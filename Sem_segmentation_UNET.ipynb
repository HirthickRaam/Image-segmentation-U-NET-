{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vii5JY170DG"
      },
      "source": [
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from glob import glob\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "SMOOTH = 1e-6\n",
        "class SemSegData(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        \n",
        "        self._init_dataset()\n",
        "        \n",
        "    def _init_dataset(self):\n",
        "\n",
        "        self.img_list = glob(self.root_dir + '/original_images/*') #creates a list of images\n",
        "        self.mask_list = glob(self.root_dir + '/label_images_semantic/*')\n",
        "        \n",
        "    \n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        img = self.img_list[index]\n",
        "        mask = self.mask_list[index]\n",
        "\n",
        "        img = (torch.tensor(np.array(Image.open(img).resize((128,128)))).permute(2,0,1) )/255 #(128,128,3)->(3,128,128)\n",
        "        \n",
        "        \n",
        "        mask = torch.unsqueeze(torch.tensor(np.array(Image.open(mask).resize((128,128),Image.NEAREST))),0) #128x128 ->1x128x128\n",
        "        \n",
        "            \n",
        "        return img, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mask_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "n00sirjkFu3d",
        "outputId": "79ff7b83-87ae-40a9-8758-a8cb50b4b4f1"
      },
      "source": [
        "obj = SemSegData(root_dir = '/content/drive/MyDrive/Colab Notebooks/semantic_drone_dataset')\n",
        "obj[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0509b8e00c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSemSegData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/semantic_drone_dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-0da6598b5212>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q09OibOI8utS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr0av903wgYN"
      },
      "source": [
        "from torchsummary import summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USO_V9NiwbVi"
      },
      "source": [
        "model = UNET()\n",
        "#model.to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIs14JKmwm3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650926bd-ba8d-4367-8c5b-6d72acd1f616"
      },
      "source": [
        "summary(model, (3,64,64))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
            "              ReLU-3           [-1, 64, 64, 64]               0\n",
            "            Conv2d-4           [-1, 64, 64, 64]          36,864\n",
            "       BatchNorm2d-5           [-1, 64, 64, 64]             128\n",
            "              ReLU-6           [-1, 64, 64, 64]               0\n",
            "        DoubleConv-7           [-1, 64, 64, 64]               0\n",
            "         MaxPool2d-8           [-1, 64, 32, 32]               0\n",
            "            Conv2d-9          [-1, 128, 32, 32]          73,728\n",
            "      BatchNorm2d-10          [-1, 128, 32, 32]             256\n",
            "             ReLU-11          [-1, 128, 32, 32]               0\n",
            "           Conv2d-12          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-13          [-1, 128, 32, 32]             256\n",
            "             ReLU-14          [-1, 128, 32, 32]               0\n",
            "       DoubleConv-15          [-1, 128, 32, 32]               0\n",
            "        MaxPool2d-16          [-1, 128, 16, 16]               0\n",
            "           Conv2d-17          [-1, 256, 16, 16]         294,912\n",
            "      BatchNorm2d-18          [-1, 256, 16, 16]             512\n",
            "             ReLU-19          [-1, 256, 16, 16]               0\n",
            "           Conv2d-20          [-1, 256, 16, 16]         589,824\n",
            "      BatchNorm2d-21          [-1, 256, 16, 16]             512\n",
            "             ReLU-22          [-1, 256, 16, 16]               0\n",
            "       DoubleConv-23          [-1, 256, 16, 16]               0\n",
            "        MaxPool2d-24            [-1, 256, 8, 8]               0\n",
            "           Conv2d-25            [-1, 512, 8, 8]       1,179,648\n",
            "      BatchNorm2d-26            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-27            [-1, 512, 8, 8]               0\n",
            "           Conv2d-28            [-1, 512, 8, 8]       2,359,296\n",
            "      BatchNorm2d-29            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-30            [-1, 512, 8, 8]               0\n",
            "       DoubleConv-31            [-1, 512, 8, 8]               0\n",
            "        MaxPool2d-32            [-1, 512, 4, 4]               0\n",
            "           Conv2d-33           [-1, 1024, 4, 4]       4,718,592\n",
            "      BatchNorm2d-34           [-1, 1024, 4, 4]           2,048\n",
            "             ReLU-35           [-1, 1024, 4, 4]               0\n",
            "           Conv2d-36           [-1, 1024, 4, 4]       9,437,184\n",
            "      BatchNorm2d-37           [-1, 1024, 4, 4]           2,048\n",
            "             ReLU-38           [-1, 1024, 4, 4]               0\n",
            "       DoubleConv-39           [-1, 1024, 4, 4]               0\n",
            "  ConvTranspose2d-40            [-1, 512, 8, 8]       2,097,664\n",
            "           Conv2d-41            [-1, 512, 8, 8]       4,718,592\n",
            "      BatchNorm2d-42            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-43            [-1, 512, 8, 8]               0\n",
            "           Conv2d-44            [-1, 512, 8, 8]       2,359,296\n",
            "      BatchNorm2d-45            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-46            [-1, 512, 8, 8]               0\n",
            "       DoubleConv-47            [-1, 512, 8, 8]               0\n",
            "  ConvTranspose2d-48          [-1, 256, 16, 16]         524,544\n",
            "           Conv2d-49          [-1, 256, 16, 16]       1,179,648\n",
            "      BatchNorm2d-50          [-1, 256, 16, 16]             512\n",
            "             ReLU-51          [-1, 256, 16, 16]               0\n",
            "           Conv2d-52          [-1, 256, 16, 16]         589,824\n",
            "      BatchNorm2d-53          [-1, 256, 16, 16]             512\n",
            "             ReLU-54          [-1, 256, 16, 16]               0\n",
            "       DoubleConv-55          [-1, 256, 16, 16]               0\n",
            "  ConvTranspose2d-56          [-1, 128, 32, 32]         131,200\n",
            "           Conv2d-57          [-1, 128, 32, 32]         294,912\n",
            "      BatchNorm2d-58          [-1, 128, 32, 32]             256\n",
            "             ReLU-59          [-1, 128, 32, 32]               0\n",
            "           Conv2d-60          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-61          [-1, 128, 32, 32]             256\n",
            "             ReLU-62          [-1, 128, 32, 32]               0\n",
            "       DoubleConv-63          [-1, 128, 32, 32]               0\n",
            "  ConvTranspose2d-64           [-1, 64, 64, 64]          32,832\n",
            "           Conv2d-65           [-1, 64, 64, 64]          73,728\n",
            "      BatchNorm2d-66           [-1, 64, 64, 64]             128\n",
            "             ReLU-67           [-1, 64, 64, 64]               0\n",
            "           Conv2d-68           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-69           [-1, 64, 64, 64]             128\n",
            "             ReLU-70           [-1, 64, 64, 64]               0\n",
            "       DoubleConv-71           [-1, 64, 64, 64]               0\n",
            "           Conv2d-72            [-1, 1, 64, 64]              65\n",
            "================================================================\n",
            "Total params: 31,037,633\n",
            "Trainable params: 31,037,633\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 58.09\n",
            "Params size (MB): 118.40\n",
            "Estimated Total Size (MB): 176.54\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBYTDJ869LLg"
      },
      "source": [
        "\"\"\"def test():\n",
        "    x = torch.randn((3, 1, 161, 161))\n",
        "    model = UNET(in_channels=1, out_channels=1)\n",
        "    preds = model(x)\n",
        "    assert preds.shape == x.shape\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "test()\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPNVJ6Y79SLd"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import os\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "#import datasets\n",
        "import random\n",
        "import numpy as np \n",
        "\n",
        "def train():\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    num_epochs = 200\n",
        "    batch_size = 8\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    \n",
        "    def soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n",
        "        \n",
        "        mask = torch.zeros((y_true.size(0),24,128,128)).to(device)\n",
        "        outs = torch.softmax(y_pred,1)\n",
        "        mask.permute(0,2,3,1).contiguous().view(-1,24)[torch.arange(y_true.size(0)*128*128),y_true.view(-1)] = 1\n",
        "        \n",
        "        numerator = 2. * (outs *mask).sum((2,3))\n",
        "        denominator = torch.sum(torch.square(outs) + torch.square(mask),(2,3))\n",
        "        \n",
        "        return torch.mean(1 - torch.mean((numerator + epsilon) / (denominator + epsilon),1)) \n",
        "\n",
        "\n",
        "    dataset = SemSegData(root_dir='/content/drive/MyDrive/Colab Notebooks/semantic_drone_dataset')\n",
        "    data_len = len(dataset)\n",
        "\n",
        "    trainset, valset = random_split(dataset, [int(0.8*data_len), (data_len - int(0.8*data_len))])\n",
        "\n",
        "    trainloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle = True)\n",
        "    valloader = DataLoader(dataset=valset, batch_size=batch_size, shuffle = False)\n",
        "\n",
        "    model = UNET().to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best = float(\"inf\")\n",
        "    train_step = 0\n",
        "    val_step = 0\n",
        "    lr_lbmd = lambda e: max(0.7**(e//20), 0.00001/0.001)\n",
        "    \n",
        "    optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    \n",
        "   \n",
        "        \n",
        "    lr_scheduler = optim.lr_scheduler.LambdaLR(optimiser, lr_lbmd)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(\"---------------\")\n",
        "        model.train()\n",
        "        loss_cntr = []\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        for idx, (img, mask) in enumerate(trainloader):\n",
        "            \n",
        "            img, mask = img.to(device), mask.to(device)\n",
        "            mask = mask.long()\n",
        "            \n",
        "            outputs = model(img)\n",
        "            \n",
        "            loss = criterion(outputs.permute(0,2,3,1).contiguous().view(-1,24),mask.view(-1,))\n",
        "            loss2 = soft_dice_loss(mask,outputs)\n",
        "            t_loss = loss + loss2\n",
        "            optimiser.zero_grad()\n",
        "            t_loss.backward(retain_graph = False)\n",
        "            optimiser.step()\n",
        "            loss_cntr.append(loss.item())\n",
        "            train_step += 1\n",
        "\n",
        "            datasets.progress_bar(progress=indx/len(trainloader), status=f\"loss: {round(np.mean(loss_cntr), 4)}\")\n",
        "        datasets.progress_bar(progress=1, status=f\"loss: {round(np.mean(loss_cntr), 4)}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        loss_cntr = []\n",
        "        model.eval()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        with torch.no_grad():\n",
        "            for idx, (img2, mask2) in enumerate(valloader):\n",
        "                \n",
        "                img2, mask2 = img2.to(device), mask2.to(device)\n",
        "                out2 = model(img2)\n",
        "                mask2 = mask2.long()\n",
        "                loss = criterion(out2.permute(0,2,3,1).contiguous().view(-1,24),mask2.view(-1,))\n",
        "                loss2 = soft_dice_loss(mask2,out2)\n",
        "            \n",
        "                loss = loss + loss2        \n",
        "                \n",
        "                loss_cntr.append(loss.item())\n",
        "                val_step += 1\n",
        "\n",
        "                datasets.progress_bar(progress=indx/len(valloader), status=f\"loss: {round(np.mean(loss_cntr), 4)}\")\n",
        "            datasets.progress_bar(progress=1, status=f\"loss: {round(np.mean(loss_cntr), 4)}\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "    train()            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISyZspScR5Ot"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Th65c1wAi6C",
        "outputId": "16e77682-bd13-4bc5-e1da-18645632a192"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}